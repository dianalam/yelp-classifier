{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Yelp's Elite: Text Processing\n",
    "Can I predict whether a Yelp user is \"elite\" or not using their activity, size of social networks, review sentiment/content, and review structure? \n",
    "\n",
    "This notebook walks through my text processing methodology for user reviews (approx 2.2 million+ total), and concludes with an analysis of the most common words used by elite users vs. non-elite users as well as a simple Naive Bayes model to demonstrate the predictive power of review text. \n",
    "\n",
    "Diana Lam  \n",
    "February 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Table of Contents**  \n",
    "[1. Load reviews data and dependencies](#1)  \n",
    "[2. Parse reviews data](#2)  \n",
    "[3. Get most common words for elite/non-elite](#3)   \n",
    "[4. Test NB model](#4)  \n",
    "[5. Export cleaned data](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='1'></a>1. Load reviews data and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from spacy.en import English, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm, tqdm_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = STOPWORDS\n",
    "punct = {p for p in string.punctuation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='2'></a>2. Parse reviews data\n",
    "\n",
    "Define functions to decode reviews and get descriptive features and tokens. \n",
    "\n",
    "Features include:\n",
    "* number of words\n",
    "* number of sentences\n",
    "* number of paragraphs\n",
    "* number of letters\n",
    "* if review mentions price\n",
    "* number of words in all caps\n",
    "* number of exclamation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(text):\n",
    "    \"\"\"Decode text.\"\"\"\n",
    "    try:\n",
    "        return text.decode('utf8')\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_words(text):\n",
    "    \"\"\"Get number of words per review.\"\"\"\n",
    "    return float(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_sents(text):\n",
    "    \"\"\"Get number of sentences per review.\"\"\"\n",
    "    # add 1 at the end for last punctuation \n",
    "    return text.count('. ') + text.count('! ') + text.count('? ') + text.count(') ') + \\\n",
    "            text.count('.\\n') + text.count('!\\n') + text.count('?\\n') + text.count(')\\n') + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_para(text):\n",
    "    \"\"\"Get number of paragraphs per review.\"\"\"\n",
    "    return text.count('\\n\\n') + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_price(text):\n",
    "    \"\"\"Check if review mentions price ($). Return 1 if yes, 0 if no.\"\"\"\n",
    "    return 1 if '$' in text else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_allcaps(text):\n",
    "    \"\"\"Get number of all uppercase words in review.\"\"\"\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    return len([word for word in text.split() if word.isupper() and len(word) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_exclamations(text):\n",
    "    \"\"\"Get number of exclamation marks in review.\"\"\"\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_chars(text):\n",
    "    \"\"\"Get number of characters in review (excluding punctuation and spaces).\"\"\"\n",
    "    return float(len([char for char in text if char != ' ' and char not in punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clean_tokens(text):  \n",
    "    \"\"\"Return tokens for each review; exclude stop words and lemmatize.\"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    words = ' '.join(letters_only.lower().split())\n",
    "    tokens = [token.lemma_ for token in nlp(words)]\n",
    "    filtered = [t for t in tokens if t not in stop and t != '' and t != ' ' and t != '\\n' and t != '\\n\\n']\n",
    "    return ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    \"\"\"Get tokens for each review in df. Implemented with tqdm to show process bar.\"\"\"\n",
    "    tokens = []\n",
    "    for i in tqdm(range(len(df.text.values))):\n",
    "        tokens.append(get_clean_tokens(df.text.values[i]))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    # decode\n",
    "    df.loc[:, 'text'] = df.loc[:, 'text'].apply(decode)\n",
    "\n",
    "    # get number of words in single review\n",
    "    df.loc[:,'review_len_wrds'] = df.loc[:,'text'].apply(get_num_words)\n",
    "\n",
    "    #get number of sentences in single review\n",
    "    df.loc[:,'review_len_sent'] = df.loc[:,'text'].apply(get_num_sents)\n",
    "\n",
    "    # get average number of words per sentence \n",
    "    df.loc[:,'avg_wrds_in_sent'] = df.loc[:,'review_len_wrds'] / df.loc[:,'review_len_sent']\n",
    "    \n",
    "    # get number of paragraphs\n",
    "    df.loc[:, 'num_para'] = df.loc[:, 'text'].apply(get_num_para)\n",
    "    \n",
    "    # check if price is mentioned\n",
    "    df.loc[:, 'mentions_price'] = df.loc[:, 'text'].apply(mentions_price)\n",
    "    \n",
    "    # get number of all caps words\n",
    "    df.loc[:, 'num_allcaps'] = df.loc[:, 'text'].apply(get_allcaps)\n",
    "    \n",
    "    # get number of exclamation marks\n",
    "    df.loc[:, 'num_exclamations'] = df.loc[:, 'text'].apply(get_exclamations)\n",
    "    \n",
    "    # get number of characters\n",
    "    df.loc[:,'num_chars'] = df.loc[:,'text'].apply(get_num_chars)\n",
    "    \n",
    "    # calculate ARI score (automatic readability index) for each review \n",
    "    df.loc[:,'ari_score'] = df.apply(\n",
    "        lambda row: 4.71 * (row.num_chars/float(row.review_len_wrds)) \\\n",
    "        + 0.5 * (row.review_len_wrds/float(row.review_len_sent)) - 21.43, \n",
    "        axis = 1)\n",
    "    \n",
    "    # get characters per word\n",
    "    df['avg_chars_per_word'] = df.loc[:,'num_chars'] / df.loc[:,'review_len_wrds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply review parsing to entire reviews dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clntkns = tokenize(reviews)\n",
    "reviews['tokens'] = clntkns\n",
    "get_features(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='3'></a>3. Get most common words for elite vs. non-elite users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load user elite/not-elite classification data and split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickled/users_elite.pkl', 'r') as picklefile:\n",
    "    userids = pickle.load(picklefile)\n",
    "    \n",
    "userids.set_index('user_id', inplace = True)\n",
    "reviews['is_elite'] = reviews.user_id.apply(lambda x: userids.loc[x, 'is_elite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_reviews = train[train.is_elite == 1]\n",
    "nonelite_reviews = train[train.is_elite == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train/test sets and vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews, test_size = .25)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None, \\\n",
    "                             max_features = 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get most popular tokens for elite and non-elite users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_features = vectorizer.fit_transform(elite_reviews.tokens)\n",
    "elite_words = vectorizer.get_feature_names()\n",
    "elite_features = elite_features.toarray()\n",
    "elite_dist = np.sum(elite_features, axis = 0)\n",
    "elite_sorted = sorted(zip(elite_words, elite_dist), key = lambda x: x[1], reverse = True)\n",
    "elite_wrds_dict = dict(elite_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonelite_features = vectorizer.fit_transform(nonelite_reviews.tokens)\n",
    "nonelite_words = vectorizer.get_feature_names()\n",
    "nonelite_features = nonelite_features.toarray()\n",
    "nonelite_dist = np.sum(nonelite_features, axis = 0)\n",
    "nonelite_sorted = sorted(zip(nonelite_words, nonelite_dist), key = lambda x: x[1], reverse = True)\n",
    "nonelite_wrds_dict = dict(nonelite_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique words (not present in intersection)\n",
    "onlyelite_words = {\n",
    "    word : elite_wrds_dict[word] \n",
    "    for word in elite_wrds_dict \n",
    "    if word not in nonelite_wrds_dict}\n",
    "\n",
    "onlynonelite_words = {\n",
    "    word : nonelite_wrds_dict[word] \n",
    "    for word in nonelite_wrds_dict \n",
    "    if word not in elite_wrds_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_top50 = sorted(onlyelite_words.items(), key = lambda x: x[1], reverse = True)[:50]\n",
    "ne_top50 = sorted(onlynonelite_words.items(), key = lambda x: x[1], reverse = True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 words used by elites but not non-elites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'locate', 19447),\n",
       " (u'space', 18615),\n",
       " (u'crispy', 16474),\n",
       " (u'butter', 15533),\n",
       " (u'le', 15208),\n",
       " (u'pepper', 15005),\n",
       " (u'dance', 14583),\n",
       " (u'grab', 14577),\n",
       " (u'mushroom', 14510),\n",
       " (u'brunch', 14379),\n",
       " (u'sound', 14018),\n",
       " (u'tender', 14004),\n",
       " (u'toast', 13732),\n",
       " (u'standard', 13707),\n",
       " (u'mall', 13668),\n",
       " (u'note', 13633),\n",
       " (u'dip', 13542),\n",
       " (u'soft', 13531),\n",
       " (u'black', 13376),\n",
       " (u'center', 13310),\n",
       " (u'et', 13263),\n",
       " (u'event', 13218),\n",
       " (u'ton', 13133),\n",
       " (u'salmon', 13075),\n",
       " (u'cute', 12819),\n",
       " (u'lobster', 12763),\n",
       " (u'cafe', 12677),\n",
       " (u'seafood', 12664),\n",
       " (u'interesting', 12542),\n",
       " (u'flavorful', 12533),\n",
       " (u'section', 12463),\n",
       " (u'sausage', 12433),\n",
       " (u'idea', 12094),\n",
       " (u'spice', 12042),\n",
       " (u'fruit', 12033),\n",
       " (u'corn', 12012),\n",
       " (u'sort', 11937),\n",
       " (u'rock', 11905),\n",
       " (u'wrap', 11834),\n",
       " (u'hang', 11803),\n",
       " (u'pour', 11765),\n",
       " (u'middle', 11738),\n",
       " (u'station', 11706),\n",
       " (u'unique', 11694),\n",
       " (u'crust', 11625),\n",
       " (u'joint', 11568),\n",
       " (u'sample', 11538),\n",
       " (u'bag', 11497),\n",
       " (u'compare', 11392),\n",
       " (u'mac', 11388)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elite_top50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 words used by non-elites but not elites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'manager', 66115),\n",
       " (u'hair', 48884),\n",
       " (u'phone', 42378),\n",
       " (u'company', 41741),\n",
       " (u'receive', 39615),\n",
       " (u'nail', 39438),\n",
       " (u'thank', 38951),\n",
       " (u'rude', 36427),\n",
       " (u'fix', 36228),\n",
       " (u'professional', 35954),\n",
       " (u'horrible', 33384),\n",
       " (u'appointment', 32467),\n",
       " (u'office', 31867),\n",
       " (u'dr', 31805),\n",
       " (u'min', 31301),\n",
       " (u'speak', 30233),\n",
       " (u'thanks', 29895),\n",
       " (u'explain', 29689),\n",
       " (u'twice', 28382),\n",
       " (u'terrible', 28275),\n",
       " (u'send', 27792),\n",
       " (u'question', 27773),\n",
       " (u'understand', 27664),\n",
       " (u'purchase', 27605),\n",
       " (u'salon', 27032),\n",
       " (u'completely', 26334),\n",
       " (u'disappointed', 25992),\n",
       " (u'massage', 25212),\n",
       " (u'die', 25132),\n",
       " (u'provide', 24828),\n",
       " (u'break', 24359),\n",
       " (u'hope', 24247),\n",
       " (u'greet', 24100),\n",
       " (u'save', 23744),\n",
       " (u'product', 23469),\n",
       " (u'boyfriend', 23468),\n",
       " (u'class', 23456),\n",
       " (u'complaint', 23384),\n",
       " (u'desk', 23061),\n",
       " (u'welcome', 22702),\n",
       " (u'state', 22597),\n",
       " (u'dollar', 22573),\n",
       " (u'woman', 22500),\n",
       " (u'deliver', 22477),\n",
       " (u'answer', 22129),\n",
       " (u'poor', 21996),\n",
       " (u'guest', 21820),\n",
       " (u'number', 21799),\n",
       " (u'daughter', 21483),\n",
       " (u'request', 21435)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_top50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='4'></a>4. Test Naive Bayes model -- predict elite/non-elite status of reviewer by review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train2 = train.dropna(subset=['tokens']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(train2.tokens)\n",
    "words = vectorizer.get_feature_names()\n",
    "features = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GaussianNB().fit(features, train2.is_elite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = vectorizer.fit_transform(test.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = test_features.toarray()\n",
    "test_pred = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.683385343266\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.72      0.77    420299\n",
      "        1.0       0.40      0.57      0.47    136005\n",
      "\n",
      "avg / total       0.73      0.68      0.70    556304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy:', accuracy_score(test.is_elite, test_pred)\n",
    "print classification_report(test.is_elite, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model(xtest, ytest):\n",
    "    return np.array(xtest.shape[0] * [ytest.mode()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_pred = baseline_model(test_features, test.is_elite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.755520362967\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.76      1.00      0.86    420299\n",
      "        1.0       0.00      0.00      0.00    136005\n",
      "\n",
      "avg / total       0.57      0.76      0.65    556304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy:', accuracy_score(test.is_elite, base_pred)\n",
    "print classification_report(test.is_elite, base_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='5'></a>5. Export data for use in classification notebook and viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add counts of elite vs. non-elite words for each review to the reviews dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_elite_words(tokens):\n",
    "    try:\n",
    "        tokens = tokens.split()\n",
    "        return len(elite_words.intersection(tokens))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_nonelite_words(tokens):\n",
    "    try:\n",
    "        tokens = tokens.split()\n",
    "        return len(ne_words.intersection(tokens))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def add_content_counts(df):\n",
    "    df['num_elite_words'] = df.text.apply(get_elite_words)\n",
    "    df['num_ne_words'] = df.text.apply(get_nonelite_words)\n",
    "\n",
    "add_content_counts(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by user and get averages for review features, then pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "byuser = reviews.groupby('user_id')\n",
    "user_avgs = byuser.mean().loc[:, 'review_len_wrds':]\n",
    "\n",
    "# pickle \n",
    "with open('pickled/user_avgs.pkl', 'w') as picklefile:\n",
    "    pickle.dump(user_avgs, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get string of words with frequency repetitions for word cloud visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# elite\n",
    "tempdict = {key : np.ceil(onlyelite_words[key]/11388.0*10) for key in onlyelite_words}\n",
    "elitewc = ' '.join([(key + ' ') * tempdict[key] for key in tempdict])\n",
    "elite_words = {item[0] for item in elite_top50}\n",
    "\n",
    "# not elite \n",
    "ne_tempdict = {key : np.ceil(onlynonelite_words[key]/21435.0*5) for key in onlynonelite_words}\n",
    "nonelitewc = ' '.join([(key + ' ') * ne_tempdict[key] for key in ne_tempdict])\n",
    "ne_words = {item[0] for item in ne_top50}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
