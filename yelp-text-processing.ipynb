{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'features: \\n- average sentence length (in words)\\n- average review length (in words)\\n- average review length (in sentences)\\n- paragraph rate\\n- bulleted or numbered list rate\\n- all caps, bad punctuation, run on sentences?\\n- bag of words: common words in elite vs. not elite; fp, fn, etc. \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"features: \n",
    "- average sentence length (in words)\n",
    "- average review length (in words)\n",
    "- average review length (in sentences)\n",
    "- paragraph rate\n",
    "- bulleted or numbered list rate\n",
    "- all caps, bad punctuation, run on sentences?\n",
    "- bag of words: common words in elite vs. not elite; fp, fn, etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string, re\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from spacy.en import English, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm, tqdm_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/yelp_academic_dataset_review.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop = STOPWORDS\n",
    "punct = {p for p in string.punctuation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to decode review and get descriptive features and tokens\n",
    "Features include:\n",
    "* number of words\n",
    "* number of sentences\n",
    "* number of paragraphs\n",
    "* number of letters\n",
    "* if review mentions price\n",
    "* number of words in all caps\n",
    "* number of exclamation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decode(text):\n",
    "    \"\"\"Decode text.\"\"\"\n",
    "    try:\n",
    "        return text.decode('utf8')\n",
    "    except:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_words(text):\n",
    "    \"\"\"Get number of words per review.\"\"\"\n",
    "    return float(len(text.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_num_sents(text):\n",
    "    \"\"\"Get number of sentences per review.\"\"\"\n",
    "    # add 1 at the end for last punctuation \n",
    "    return text.count('. ') + text.count('! ') + text.count('? ') + text.count(') ') + \\\n",
    "            text.count('.\\n') + text.count('!\\n') + text.count('?\\n') + text.count(')\\n') + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_para(text):\n",
    "    \"\"\"Get number of paragraphs per review.\"\"\"\n",
    "    return text.count('\\n\\n') + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_price(text):\n",
    "    \"\"\"Check if review mentions price ($). Return 1 if yes, 0 if no.\"\"\"\n",
    "    return 1 if '$' in text else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_allcaps(text):\n",
    "    \"\"\"Get number of all uppercase words in review.\"\"\"\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    return len([word for word in text.split() if word.isupper() and len(word) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_exclamations(text):\n",
    "    \"\"\"Get number of exclamation marks in review.\"\"\"\n",
    "    return text.count('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_chars(text):\n",
    "    \"\"\"Get number of characters in review (excluding punctuation and spaces).\"\"\"\n",
    "    return float(len([char for char in text if char != ' ' and char not in punct]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_clean_tokens(text):  \n",
    "    \"\"\"Return tokens for each review; exclude stop words and lemmatize.\"\"\"\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text) \n",
    "    words = ' '.join(letters_only.lower().split())\n",
    "    tokens = [token.lemma_ for token in nlp(words)]\n",
    "    filtered = [t for t in tokens if t not in stop and t != '' and t != ' ' and t != '\\n' and t != '\\n\\n']\n",
    "    return ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    \"\"\"Get tokens for each review in df. Implemented with tqdm to show process bar.\"\"\"\n",
    "    tokens = []\n",
    "    for i in tqdm(range(len(df.text.values))):\n",
    "        tokens.append(get_clean_tokens(df.text.values[i]))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1071,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "clntkns = tokenize(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1075,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('pickled/tokens.pkl', 'w') as picklefile:\n",
    "#     pickle.dump(clntkns, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse reviews df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    # decode\n",
    "    df.loc[:, 'text'] = df.loc[:, 'text'].apply(decode)\n",
    "\n",
    "    # get number of words in single review\n",
    "    df.loc[:,'review_len_wrds'] = df.loc[:,'text'].apply(get_num_words)\n",
    "\n",
    "    #get number of sentences in single review\n",
    "    df.loc[:,'review_len_sent'] = df.loc[:,'text'].apply(get_num_sents)\n",
    "\n",
    "    # get average number of words per sentence \n",
    "    df.loc[:,'avg_wrds_in_sent'] = df.loc[:,'review_len_wrds'] / df.loc[:,'review_len_sent']\n",
    "    \n",
    "    # get number of paragraphs\n",
    "    df.loc[:, 'num_para'] = df.loc[:, 'text'].apply(get_num_para)\n",
    "    \n",
    "    # check if price is mentioned\n",
    "    df.loc[:, 'mentions_price'] = df.loc[:, 'text'].apply(mentions_price)\n",
    "    \n",
    "    # get number of all caps words\n",
    "    df.loc[:, 'num_allcaps'] = df.loc[:, 'text'].apply(get_allcaps)\n",
    "    \n",
    "    # get number of exclamation marks\n",
    "    df.loc[:, 'num_exclamations'] = df.loc[:, 'text'].apply(get_exclamations)\n",
    "    \n",
    "    # get number of characters\n",
    "    df.loc[:,'num_chars'] = df.loc[:,'text'].apply(get_num_chars)\n",
    "    \n",
    "    # calculate ARI score (automatic readability index) for each review \n",
    "    df.loc[:,'ari_score'] = df.apply(\n",
    "        lambda row: 4.71 * (row.num_chars/float(row.review_len_wrds)) \\\n",
    "        + 0.5 * (row.review_len_wrds/float(row.review_len_sent)) - 21.43, \n",
    "        axis = 1)\n",
    "    \n",
    "    # get characters per word\n",
    "    df['avg_chars_per_word'] = df.loc[:,'num_chars'] / df.loc[:,'review_len_wrds']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add content counts - move this stuff to after text processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add elite/nonelite words\n",
    "\n",
    "def get_elite_words(tokens):\n",
    "    try:\n",
    "        tokens = tokens.split()\n",
    "        return len(elite_words.intersection(tokens))\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_nonelite_words(tokens):\n",
    "    try:\n",
    "        tokens = tokens.split()\n",
    "        return len(ne_words.intersection(tokens))\n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_content_counts(df):\n",
    "    df['num_elite_words'] = df.text.apply(get_elite_words)\n",
    "    df['num_ne_words'] = df.text.apply(get_nonelite_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "add_content_counts(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### /end add content counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_chars_per_word(reviews)\n",
    "get_features(reviews)\n",
    "get_more_features(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "byuser = reviews.groupby('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_avgs = byuser.mean().loc[:, 'review_len_wrds':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# can delete, captured above \n",
    "# user_content = byuser.mean().loc[:, 'num_elite_words':'num_ne_words']\n",
    "# user_ari = byuser.mean().loc[:,'ari_score']\n",
    "# user_wrd_length = byuser.mean().loc[:,'avg_chars_per_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle user words/content data\n",
    "with open('pickled/user_content.pkl', 'w') as picklefile:\n",
    "    pickle.dump(user_content, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pickle user avgs data\n",
    "with open('pickled/user_avgs.pkl', 'w') as picklefile:\n",
    "    pickle.dump(user_avgs, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle user avg ari data\n",
    "with open('pickled/user_ari.pkl', 'w') as picklefile:\n",
    "    pickle.dump(user_ari, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle user avg word length data\n",
    "with open('pickled/user_wrdlength.pkl', 'w') as picklefile:\n",
    "    pickle.dump(user_wrd_length, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews['tokens'] = clntkns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### workspace for pickling and loading entire df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickled/reviewsdf2.pkl', 'w') as picklefile:\n",
    "    pickle.dump(reviews, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickled/reviewsdf2.pkl', 'r') as picklefile:\n",
    "    reviews = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most common words for elite vs. non-elite users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pickled/users_elite.pkl', 'r') as picklefile:\n",
    "    userids = pickle.load(picklefile)\n",
    "    \n",
    "userids.set_index('user_id', inplace = True)\n",
    "reviews['is_elite'] = reviews.user_id.apply(lambda x: userids.loc[x, 'is_elite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_reviews = train[train.is_elite == 1]\n",
    "nonelite_reviews = train[train.is_elite == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare train/test sets and vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(reviews, test_size = .25)\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None, \\\n",
    "                             max_features = 500) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get most popular tokens for elite and non-elite users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_features = vectorizer.fit_transform(elite_reviews.tokens)\n",
    "elite_words = vectorizer.get_feature_names()\n",
    "elite_features = elite_features.toarray()\n",
    "elite_dist = np.sum(elite_features, axis = 0)\n",
    "elite_sorted = sorted(zip(elite_words, elite_dist), key = lambda x: x[1], reverse = True)\n",
    "elite_wrds_dict = dict(elite_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonelite_features = vectorizer.fit_transform(nonelite_reviews.tokens)\n",
    "nonelite_words = vectorizer.get_feature_names()\n",
    "nonelite_features = nonelite_features.toarray()\n",
    "nonelite_dist = np.sum(nonelite_features, axis = 0)\n",
    "nonelite_sorted = sorted(zip(nonelite_words, nonelite_dist), key = lambda x: x[1], reverse = True)\n",
    "nonelite_wrds_dict = dict(nonelite_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get unique words (not present in intersection)\n",
    "onlyelite_words = {\n",
    "    word : elite_wrds_dict[word] \n",
    "    for word in elite_wrds_dict \n",
    "    if word not in nonelite_wrds_dict}\n",
    "\n",
    "onlynonelite_words = {\n",
    "    word : nonelite_wrds_dict[word] \n",
    "    for word in nonelite_wrds_dict \n",
    "    if word not in elite_wrds_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elite_top50 = sorted(onlyelite_words.items(), key = lambda x: x[1], reverse = True)[:50]\n",
    "ne_top50 = sorted(onlynonelite_words.items(), key = lambda x: x[1], reverse = True)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 words used by elites but not non-elites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'locate', 19447),\n",
       " (u'space', 18615),\n",
       " (u'crispy', 16474),\n",
       " (u'butter', 15533),\n",
       " (u'le', 15208),\n",
       " (u'pepper', 15005),\n",
       " (u'dance', 14583),\n",
       " (u'grab', 14577),\n",
       " (u'mushroom', 14510),\n",
       " (u'brunch', 14379),\n",
       " (u'sound', 14018),\n",
       " (u'tender', 14004),\n",
       " (u'toast', 13732),\n",
       " (u'standard', 13707),\n",
       " (u'mall', 13668),\n",
       " (u'note', 13633),\n",
       " (u'dip', 13542),\n",
       " (u'soft', 13531),\n",
       " (u'black', 13376),\n",
       " (u'center', 13310),\n",
       " (u'et', 13263),\n",
       " (u'event', 13218),\n",
       " (u'ton', 13133),\n",
       " (u'salmon', 13075),\n",
       " (u'cute', 12819),\n",
       " (u'lobster', 12763),\n",
       " (u'cafe', 12677),\n",
       " (u'seafood', 12664),\n",
       " (u'interesting', 12542),\n",
       " (u'flavorful', 12533),\n",
       " (u'section', 12463),\n",
       " (u'sausage', 12433),\n",
       " (u'idea', 12094),\n",
       " (u'spice', 12042),\n",
       " (u'fruit', 12033),\n",
       " (u'corn', 12012),\n",
       " (u'sort', 11937),\n",
       " (u'rock', 11905),\n",
       " (u'wrap', 11834),\n",
       " (u'hang', 11803),\n",
       " (u'pour', 11765),\n",
       " (u'middle', 11738),\n",
       " (u'station', 11706),\n",
       " (u'unique', 11694),\n",
       " (u'crust', 11625),\n",
       " (u'joint', 11568),\n",
       " (u'sample', 11538),\n",
       " (u'bag', 11497),\n",
       " (u'compare', 11392),\n",
       " (u'mac', 11388)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elite_top50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 50 words used by non-elites but not elites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'manager', 66115),\n",
       " (u'hair', 48884),\n",
       " (u'phone', 42378),\n",
       " (u'company', 41741),\n",
       " (u'receive', 39615),\n",
       " (u'nail', 39438),\n",
       " (u'thank', 38951),\n",
       " (u'rude', 36427),\n",
       " (u'fix', 36228),\n",
       " (u'professional', 35954),\n",
       " (u'horrible', 33384),\n",
       " (u'appointment', 32467),\n",
       " (u'office', 31867),\n",
       " (u'dr', 31805),\n",
       " (u'min', 31301),\n",
       " (u'speak', 30233),\n",
       " (u'thanks', 29895),\n",
       " (u'explain', 29689),\n",
       " (u'twice', 28382),\n",
       " (u'terrible', 28275),\n",
       " (u'send', 27792),\n",
       " (u'question', 27773),\n",
       " (u'understand', 27664),\n",
       " (u'purchase', 27605),\n",
       " (u'salon', 27032),\n",
       " (u'completely', 26334),\n",
       " (u'disappointed', 25992),\n",
       " (u'massage', 25212),\n",
       " (u'die', 25132),\n",
       " (u'provide', 24828),\n",
       " (u'break', 24359),\n",
       " (u'hope', 24247),\n",
       " (u'greet', 24100),\n",
       " (u'save', 23744),\n",
       " (u'product', 23469),\n",
       " (u'boyfriend', 23468),\n",
       " (u'class', 23456),\n",
       " (u'complaint', 23384),\n",
       " (u'desk', 23061),\n",
       " (u'welcome', 22702),\n",
       " (u'state', 22597),\n",
       " (u'dollar', 22573),\n",
       " (u'woman', 22500),\n",
       " (u'deliver', 22477),\n",
       " (u'answer', 22129),\n",
       " (u'poor', 21996),\n",
       " (u'guest', 21820),\n",
       " (u'number', 21799),\n",
       " (u'daughter', 21483),\n",
       " (u'request', 21435)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne_top50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get string of words with frequency repetitions for word cloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# elite\n",
    "tempdict = {key : np.ceil(onlyelite_words[key]/11388.0*10) for key in onlyelite_words}\n",
    "elitewc = ' '.join([(key + ' ') * tempdict[key] for key in tempdict])\n",
    "elite_words = {item[0] for item in elite_top50}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# not elite \n",
    "ne_tempdict = {key : np.ceil(onlynonelite_words[key]/21435.0*5) for key in onlynonelite_words}\n",
    "nonelitewc = ' '.join([(key + ' ') * ne_tempdict[key] for key in ne_tempdict])\n",
    "ne_words = {item[0] for item in ne_top50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run bag of words naive bayes model to predict elite/non-elite status of reviewer by review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train2 = train.dropna(subset=['tokens']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(train2.tokens)\n",
    "words = vectorizer.get_feature_names()\n",
    "features = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GaussianNB().fit(features, train2.is_elite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = vectorizer.fit_transform(test.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = test_features.toarray()\n",
    "test_pred = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.683385343266\n",
      "0.397691174146\n",
      "0.573464210875\n"
     ]
    }
   ],
   "source": [
    "print accuracy_score(test.is_elite, test_pred)\n",
    "print precision_score(test.is_elite, test_pred)\n",
    "print recall_score(test.is_elite, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model(xtest, ytest):\n",
    "    return np.array(xtest.shape[0] * [ytest.mode()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_pred = baseline_model(test_features, test.is_elite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75549519687077571"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.is_elite.value_counts()[0] / float(test.is_elite.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
